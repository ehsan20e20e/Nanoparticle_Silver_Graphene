"""
The code for the research presented in the paper titled "A_deep_learning_method_for_extinction_spectrum_prediction_and_graphene-coated_silver_nanoparticles_inverse_design"

This code corresponds to the article's invers Deep Neural Network (DNN) section.
Please cite the paper in any publication using this code.
"""
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0209d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Setup\n",
    "# ===========================\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    print(\"‚úÖ TensorFlow imported successfully\")\n",
    "    \n",
    "   \n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            \n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"‚úÖ GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è GPU configuration error: {e}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Installing TensorFlow...\")\n",
    "    !pip install -q tensorflow==2.18.0\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ÿ™ŸÜÿ∏€åŸÖ seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ===========================\n",
    "# Paths\n",
    "# ===========================\n",
    "BASE_DIR = Path(\"./nn_inverse_ga_outputs\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = Path(\"C:\\\\Users\\\\hoseini\\\\Desktop\\\\merge-csv.com__68d6b5a27ee3c.csv\")\n",
    "TEST_CSV = Path(\"C:\\\\Users\\\\hoseini\\\\Desktop\\\\Test_Forward_model.csv\") \n",
    "\n",
    "MODEL_DIR = BASE_DIR / \"best_model\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "PLOTS_DIR = BASE_DIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "ARTIFACTS = BASE_DIR / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# ===========================\n",
    "# Data Loading & Preprocessing for INVERSE\n",
    "# ===========================\n",
    "def load_and_prepare_inverse_data(csv_path: Path, sequence_length=600):\n",
    "    \"\"\"upload and prepare dataset\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    \n",
    "    print(\"data structure:\")\n",
    "    print(df.head())\n",
    "    print(f\"number of rows: {len(df)}\")\n",
    "    print(f\"column: {df.columns.tolist()}\")\n",
    "    \n",
    "   \n",
    "    if len(df.columns) >= 4:\n",
    "        df.columns = ['index', 'param1', 'param2', 'output']\n",
    "    else:\n",
    "       \n",
    "        df.columns = ['param1', 'param2', 'output'] + [f'extra_{i}' for i in range(len(df.columns)-3)]\n",
    "    \n",
    "   \n",
    "    def preprocess_outputs(outputs):\n",
    "        outputs = np.array(outputs, dtype=float)\n",
    "        outputs = np.abs(outputs)  \n",
    "        outputs = np.clip(outputs, 0, 1)  \n",
    "        return outputs\n",
    "    \n",
    "    df['output_processed'] = preprocess_outputs(df['output'].values)\n",
    "    \n",
    "    \n",
    "    X_inverse, y_inverse = [], []\n",
    "    \n",
    "    \n",
    "    grouped = df.groupby(['param1', 'param2'])\n",
    "    \n",
    "    for (param1, param2), group in grouped:\n",
    "        \n",
    "        if 'index' in group.columns:\n",
    "            group = group.sort_values('index')\n",
    "        else:\n",
    "            group = group.sort_index()\n",
    "        \n",
    "        outputs = group['output_processed'].values\n",
    "        \n",
    "        \n",
    "        if len(outputs) >= sequence_length:\n",
    "          \n",
    "            for i in range(0, len(outputs) - sequence_length + 1, sequence_length):\n",
    "                X_inverse.append(outputs[i:i+sequence_length])\n",
    "                y_inverse.append([param1, param2])\n",
    "    \n",
    "    X_inverse = np.array(X_inverse)\n",
    "    y_inverse = np.array(y_inverse)\n",
    "    \n",
    "    print(f\"invers data: {X_inverse.shape} -> {y_inverse.shape}\")\n",
    "    print(f\"number of samples: {len(X_inverse)}\")\n",
    "    return X_inverse, y_inverse, df\n",
    "\n",
    "\n",
    "X_all, y_all, df_original = load_and_prepare_inverse_data(TRAIN_CSV, sequence_length=600)\n",
    "\n",
    "if len(X_all) == 0:\n",
    "    print(\"‚ùå not enough data!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all, y_all, test_size=0.10, random_state=42, shuffle=False  \n",
    ")\n",
    "\n",
    "print(f\"training data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"validation data : {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# Preprocessing for INVERSE\n",
    "# ===========================\n",
    "def preprocess_inverse_inputs(X_train, X_val):\n",
    "    \"\"\" preprocessing invers input data\"\"\"\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled, scaler_X\n",
    "\n",
    "def preprocess_inverse_outputs(y_train, y_val):\n",
    "    \"\"\"preprocessing invers output data\"\"\"\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    return y_train_scaled, y_val_scaled, scaler_y\n",
    "\n",
    "\n",
    "X_train_p, X_val_p, scaler_X = preprocess_inverse_inputs(X_train, X_val)\n",
    "y_train_p, y_val_p, scaler_y = preprocess_inverse_outputs(y_train, y_val)\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# ===========================\n",
    "def build_inverse_model(input_dim, cfg):\n",
    "    \"\"\"creat invers modle\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    n_layers = cfg[\"n_layers\"]\n",
    "    units_list = cfg[\"units_per_layer\"]\n",
    "    \n",
    "    \n",
    "    if len(units_list) != n_layers:\n",
    "        units_list = [512, 256, 128] \n",
    "    \n",
    "    \n",
    "    model.add(layers.Dense(units_list[0], activation=cfg[\"activation\"], \n",
    "                          kernel_regularizer=regularizers.l2(cfg[\"l2\"]) if cfg[\"l2\"] > 0 else None,\n",
    "                          input_shape=(input_dim,)))\n",
    "    if cfg[\"dropout\"] > 0:\n",
    "        model.add(layers.Dropout(cfg[\"dropout\"]))\n",
    "    \n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        units = units_list[i]\n",
    "        model.add(layers.Dense(units, activation=cfg[\"activation\"],\n",
    "                              kernel_regularizer=regularizers.l2(cfg[\"l2\"]) if cfg[\"l2\"] > 0 else None))\n",
    "        if cfg[\"dropout\"] > 0:\n",
    "            model.add(layers.Dropout(cfg[\"dropout\"]))\n",
    "    \n",
    "    \n",
    "    model.add(layers.Dense(2, activation=\"linear\"))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=cfg[\"lr\"])\n",
    "    model.compile(optimizer=opt, loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# ===========================\n",
    "ACTIVATIONS = [\"relu\", \"tanh\", \"elu\"]\n",
    "\n",
    "def random_inverse_config():\n",
    "    \"\"\"random setting\"\"\"\n",
    "    n_layers = random.randint(3, 6)  \n",
    "    \n",
    "     \n",
    "    if n_layers == 3:\n",
    "        units = [random.choice([512, 600, 700]), 256, 128]\n",
    "    elif n_layers == 4:\n",
    "        units = [random.choice([600, 700, 800]), 400, 200, 100]\n",
    "    elif n_layers == 5:\n",
    "        units = [random.choice([700, 800, 900]), 500, 300, 150, 75]\n",
    "    else:  # 6 layers\n",
    "        units = [random.choice([800, 900, 1024]), 600, 400, 200, 100, 50]\n",
    "    \n",
    "    return {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units_per_layer\": units,\n",
    "        \"activation\": random.choice(ACTIVATIONS),\n",
    "        \"dropout\": round(random.uniform(0.1, 0.4), 2),\n",
    "        \"l2\": round(random.choice([1e-5, 1e-4, 5e-4]), 6),\n",
    "        \"lr\": 10**random.uniform(-4, -3),\n",
    "        \"batch_size\": random.choice([8, 16, 32]),  \n",
    "        \"epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "    }\n",
    "\n",
    "def mutate_inverse(cfg, p=0.3):\n",
    "    \"\"\"jump for invers\"\"\"\n",
    "    new_cfg = json.loads(json.dumps(cfg))\n",
    "    \n",
    "    if random.random() < p:\n",
    "        new_cfg[\"activation\"] = random.choice(ACTIVATIONS)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"dropout\"] = round(np.clip(new_cfg[\"dropout\"] + random.uniform(-0.1, 0.1), 0.05, 0.5), 2)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"l2\"] = round(random.choice([1e-5, 1e-4, 5e-4]), 6)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"lr\"] = 10**random.uniform(-4, -3)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"batch_size\"] = random.choice([8, 16, 32])\n",
    "    if random.random() < p:\n",
    "        nl = random.randint(3, 6)\n",
    "        \n",
    "        if nl == 3:\n",
    "            units = [random.choice([512, 600, 700]), 256, 128]\n",
    "        elif nl == 4:\n",
    "            units = [random.choice([600, 700, 800]), 400, 200, 100]\n",
    "        elif nl == 5:\n",
    "            units = [random.choice([700, 800, 900]), 500, 300, 150, 75]\n",
    "        else:  # 6 layers\n",
    "            units = [random.choice([800, 900, 1024]), 600, 400, 200, 100, 50]\n",
    "        \n",
    "        new_cfg[\"n_layers\"] = nl\n",
    "        new_cfg[\"units_per_layer\"] = units\n",
    "    \n",
    "    return new_cfg\n",
    "\n",
    "def crossover_inverse(a, b):\n",
    "   \n",
    "    child = {}\n",
    "    for k in a.keys():\n",
    "        child[k] = random.choice([a[k], b[k]])\n",
    "    \n",
    "   \n",
    "    nl = child[\"n_layers\"]\n",
    "    units = child[\"units_per_layer\"]\n",
    "    \n",
    "    if len(units) != nl:\n",
    "        \n",
    "        if nl == 3:\n",
    "            new_units = [random.choice([512, 600, 700]), 256, 128]\n",
    "        elif nl == 4:\n",
    "            new_units = [random.choice([600, 700, 800]), 400, 200, 100]\n",
    "        elif nl == 5:\n",
    "            new_units = [random.choice([700, 800, 900]), 500, 300, 150, 75]\n",
    "        else:  # 6 layers\n",
    "            new_units = [random.choice([800, 900, 1024]), 600, 400, 200, 100, 50]\n",
    "        \n",
    "        child[\"units_per_layer\"] = new_units\n",
    "    \n",
    "    return child\n",
    "\n",
    "def evaluate_inverse_cfg(cfg, X_tr, y_tr, X_va, y_va):\n",
    "    \"\"\"evaluation configuration \"\"\"\n",
    "    try:\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = build_inverse_model(X_tr.shape[1], cfg)\n",
    "        cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=cfg[\"patience\"], restore_best_weights=True)]\n",
    "        \n",
    "        hist = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=cfg[\"epochs\"],\n",
    "            batch_size=cfg[\"batch_size\"],\n",
    "            verbose=0,\n",
    "            callbacks=cb\n",
    "        )\n",
    "        \n",
    "        val_mse = float(min(hist.history[\"val_loss\"]))\n",
    "        best_epoch = int(np.argmin(hist.history[\"val_loss\"])) + 1\n",
    "        \n",
    "        \n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        return val_mse, best_epoch\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error in avaluation configuration : {e}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        return float('inf'), 0\n",
    "\n",
    "# ===========================\n",
    "# Run GA for INVERSE \n",
    "# ===========================\n",
    "POP_SIZE, N_GEN, ELITE_K, MUT_P = 12, 8, 3, 0.4\n",
    "population = [random_inverse_config() for _ in range(POP_SIZE)]\n",
    "hall = []\n",
    "\n",
    "print(\"start\")\n",
    "print(f\"p: {POP_SIZE}, g: {N_GEN}\")\n",
    "\n",
    "for gen in range(N_GEN):\n",
    "    scored = []\n",
    "    for i, cfg in enumerate(population):\n",
    "        print(f\"  eval modle {i+1}/{len(population)} in generation {gen+1}...\")\n",
    "        val_mse, best_epoch = evaluate_inverse_cfg(cfg, X_train_p, y_train_p, X_val_p, y_val_p)\n",
    "        if val_mse < float('inf'):\n",
    "            scored.append((val_mse, best_epoch, cfg))\n",
    "            print(f\"    MSE: {val_mse:.6f}, layers: {cfg['n_layers']}, units: {cfg['units_per_layer'][:2]}...\")\n",
    "        else:\n",
    "            print(f\"    ‚ùå error in eval\")\n",
    "    \n",
    "    if not scored:\n",
    "        print(\"no configuration\")\n",
    "        population = [random_inverse_config() for _ in range(POP_SIZE)]\n",
    "        continue\n",
    "        \n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    best_gen = scored[0]\n",
    "    hall.append({\"gen\": gen, \"val_mse\": best_gen[0], \"best_epoch\": best_gen[1], \"cfg\": best_gen[2]})\n",
    "    \n",
    "    print(f\"[Gen {gen+1}] best val MSE = {best_gen[0]:.6f} | layers={best_gen[2]['n_layers']} | units={best_gen[2]['units_per_layer']}\")\n",
    "    \n",
    "    new_pop = [scored[i][2] for i in range(min(ELITE_K, len(scored)))]\n",
    "    \n",
    "    while len(new_pop) < POP_SIZE:\n",
    "        if len(scored) >= 2:\n",
    "            a, b = random.sample(scored[:min(4, len(scored))], 2)\n",
    "            child = crossover_inverse(a[2], b[2])\n",
    "            child = mutate_inverse(child, p=MUT_P)\n",
    "            new_pop.append(child)\n",
    "        else:\n",
    "            new_pop.append(random_inverse_config())\n",
    "    \n",
    "    population = new_pop\n",
    "\n",
    "if hall:\n",
    "    hall.sort(key=lambda h: h[\"val_mse\"])\n",
    "    best_cfg = hall[0][\"cfg\"]\n",
    "    print(\"\\nüéØ best configuration :\")\n",
    "    print(json.dumps(best_cfg, indent=2))\n",
    "    \n",
    "    with open(ARTIFACTS / \"best_inverse_config.json\", \"w\") as f:\n",
    "        json.dump(best_cfg, f, indent=2)\n",
    "\n",
    "    # ===========================\n",
    "    # Final Training on best config for INVERSE\n",
    "    # ===========================\n",
    "    print(\"\\nüî• training with the best configuration...\")\n",
    "    best_model = build_inverse_model(X_train_p.shape[1], best_cfg)\n",
    "    \n",
    "    cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=best_cfg[\"patience\"], restore_best_weights=True)]\n",
    "    \n",
    "    hist = best_model.fit(\n",
    "        X_train_p, y_train_p,\n",
    "        validation_data=(X_val_p, y_val_p),\n",
    "        epochs=best_cfg[\"epochs\"],\n",
    "        batch_size=best_cfg[\"batch_size\"],\n",
    "        verbose=1,\n",
    "        callbacks=cb\n",
    "    )\n",
    "    \n",
    "    best_model.save(MODEL_DIR / \"trained_inverse_model.h5\")\n",
    "    pd.DataFrame(hist.history).to_csv(ARTIFACTS / \"inverse_train_history.csv\", index=False)\n",
    "   \n",
    "    joblib.dump(scaler_X, MODEL_DIR / \"inverse_scaler_X.pkl\")\n",
    "    joblib.dump(scaler_y, MODEL_DIR / \"inverse_scaler_y.pkl\")\n",
    "\n",
    "    # ===========================\n",
    "    # Plots for INVERSE\n",
    "    # ===========================\n",
    "    def save_tiff_plot(filename, fig):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(PLOTS_DIR / filename, dpi=300, format=\"tiff\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist.history[\"loss\"], label=\"Train MSE\")\n",
    "    plt.plot(hist.history[\"val_loss\"], label=\"Validation MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Training vs Validation MSE (Inverse Model)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(hist.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"Training vs Validation MAE (Inverse Model)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    save_tiff_plot(\"inverse_mse_mae_train_val.tiff\", fig1)\n",
    "\n",
    "    # ===========================\n",
    "    # Test Evaluation for INVERSE\n",
    "    # ===========================\n",
    "    def load_inverse_test(csv_path: Path, sequence_length=600):\n",
    "        \"\"\"upload test dataset\"\"\"\n",
    "        X_test, y_test, _ = load_and_prepare_inverse_data(csv_path, sequence_length)\n",
    "        \n",
    "        if len(X_test) == 0:\n",
    "            print(\"‚ùå not enough data!\")\n",
    "            return None, None\n",
    "            \n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_test_original = y_test  \n",
    "        \n",
    "        return X_test_scaled, y_test_original\n",
    "\n",
    "    if TEST_CSV.exists():\n",
    "        print(\"test\")\n",
    "        X_test_p, y_test_true = load_inverse_test(TEST_CSV)\n",
    "        \n",
    "        if X_test_p is not None:\n",
    "            y_test_pred_scaled = best_model.predict(X_test_p, verbose=0)\n",
    "            y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)\n",
    "            \n",
    "            \n",
    "            mse_param1 = mean_squared_error(y_test_true[:, 0], y_test_pred[:, 0])\n",
    "            mse_param2 = mean_squared_error(y_test_true[:, 1], y_test_pred[:, 1])\n",
    "            total_mse = (mse_param1 + mse_param2) / 2\n",
    "            \n",
    "            \n",
    "            fig2 = plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_test_true[:, 0], y_test_pred[:, 0], alpha=0.7, s=50)\n",
    "            min_val = min(y_test_true[:, 0].min(), y_test_pred[:, 0].min())\n",
    "            max_val = max(y_test_true[:, 0].max(), y_test_pred[:, 0].max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "            plt.xlabel('real value Param1')\n",
    "            plt.ylabel('predicted value Param1')\n",
    "            plt.title(f'Param1 - MSE: {mse_param1:.6f}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test_true[:, 1], y_test_pred[:, 1], alpha=0.7, s=50)\n",
    "            min_val = min(y_test_true[:, 1].min(), y_test_pred[:, 1].min())\n",
    "            max_val = max(y_test_true[:, 1].max(), y_test_pred[:, 1].max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "            plt.xlabel('real value Param2')\n",
    "            plt.ylabel('predicted value Param2')\n",
    "            plt.title(f'Param2 - MSE: {mse_param2:.6f}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            save_tiff_plot(\"inverse_test_scatter.tiff\", fig2)\n",
    "            \n",
    "            # ŸÜŸÖÿß€åÿ¥ ŸÜŸÖŸàŸÜŸá‚Äåÿß€å ÿßÿ≤ ŸÜÿ™ÿß€åÿ¨\n",
    "            print(f\"\\n sample of test:\")\n",
    "            for i in range(min(5, len(y_test_true))):\n",
    "                print(f\"  sample {i+1}:\")\n",
    "                print(f\"    real:    Param1={y_test_true[i, 0]:.3f}, Param2={y_test_true[i, 1]:.3f}\")\n",
    "                print(f\"    prediction: Param1={y_test_pred[i, 0]:.3f}, Param2={y_test_pred[i, 1]:.3f}\")\n",
    "                print(f\"    error:     Param1={abs(y_test_true[i, 0] - y_test_pred[i, 0]):.3f}, Param2={abs(y_test_true[i, 1] - y_test_pred[i, 1]):.3f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è no test dataset\")\n",
    "        total_mse = hall[0][\"val_mse\"]\n",
    "\n",
    "    # ===========================\n",
    "    # Final Report JSON for INVERSE\n",
    "    # ===========================\n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_type\": \"inverse\",\n",
    "        \"input_description\": \"600 consecutive output values\",\n",
    "        \"output_description\": \"2 parameters [param1, param2]\",\n",
    "        \"train_size\": int(X_train.shape[0]),\n",
    "        \"val_size\": int(X_val.shape[0]),\n",
    "        \"test_size\": int(len(y_test_true)) if 'y_test_true' in locals() else 0,\n",
    "        \"input_dimension\": int(X_train.shape[1]),\n",
    "        \"output_dimension\": 2,\n",
    "        \"preprocessing\": {\n",
    "            \"input_scaling\": \"StandardScaler\",\n",
    "            \"output_scaling\": \"StandardScaler\",\n",
    "            \"output_preprocessing\": \"abs() + clip(0,1)\"\n",
    "        },\n",
    "        \"best_config\": best_cfg,\n",
    "        \"best_val_mse\": float(min(hist.history[\"val_loss\"])),\n",
    "        \"early_stopped_epoch\": int(np.argmin(hist.history[\"val_loss\"])) + 1,\n",
    "        \"test_mse\": float(total_mse) if 'total_mse' in locals() else None,\n",
    "        \"artifacts\": {\n",
    "            \"model_h5\": str((MODEL_DIR / \"trained_inverse_model.h5\").resolve()),\n",
    "            \"scaler_X\": str((MODEL_DIR / \"inverse_scaler_X.pkl\").resolve()),\n",
    "            \"scaler_y\": str((MODEL_DIR / \"inverse_scaler_y.pkl\").resolve()),\n",
    "            \"best_config_json\": str((ARTIFACTS / \"best_inverse_config.json\").resolve()),\n",
    "            \"train_history_csv\": str((ARTIFACTS / \"inverse_train_history.csv\").resolve()),\n",
    "            \"mse_plot_tiff\": str((PLOTS_DIR / \"inverse_mse_mae_train_val.tiff\").resolve()),\n",
    "            \"test_plot_tiff\": str((PLOTS_DIR / \"inverse_test_scatter.tiff\").resolve() if TEST_CSV.exists() else \"N/A\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(BASE_DIR / \"inverse_summary_report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ complate traning!\")\n",
    "    if 'total_mse' in locals():\n",
    "        print(f\"üìä MSE test: {total_mse:.6f}\")\n",
    "    \n",
    "    # ===========================\n",
    "    # Prediction Function for INVERSE\n",
    "    # ===========================\n",
    "    def predict_inverse(outputs_600):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        outputs_600: list of 600 outputs\n",
    "        \n",
    "        Returns:\n",
    "        predicted parameters [param1, param2]\n",
    "        \"\"\"\n",
    "\n",
    "        processed_input = np.abs(np.array(outputs_600, dtype=float))\n",
    "        processed_input = np.clip(processed_input, 0, 1)\n",
    "        \n",
    "  \n",
    "        input_scaled = scaler_X.transform([processed_input])\n",
    "        \n",
    "\n",
    "        pred_scaled = best_model.predict(input_scaled, verbose=0)\n",
    "        prediction = scaler_y.inverse_transform(pred_scaled)[0]\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "   \n",
    "    print(\"\"\"\n",
    "predicted_params = predict_inverse(outputs_600)\n",
    "print(f\"predicted parameter: {predicted_params}\")\n",
    "    \"\"\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå no configuration !\")\n",
    "\n",
    "print(\"üéØ finish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
