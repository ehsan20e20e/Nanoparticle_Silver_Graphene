"""
The code for the research presented in the paper titled "A_deep_learning_method_for_extinction_spectrum_prediction_and_graphene-coated_silver_nanoparticles_inverse_design"

This code corresponds to the article's forward Deep Neural Network (DNN) section.
Please cite the paper in any publication using this code.
"""
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Setup\n",
    "# ===========================\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    print(\"✅ TensorFlow imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Installing TensorFlow...\")\n",
    "    !pip install -q tensorflow==2.18.0\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# تنظیم seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ===========================\n",
    "# Paths\n",
    "# ===========================\n",
    "BASE_DIR = Path(\"./nn_regression_ga_outputs\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = Path(\"C:\\\\Users\\\\hoseini\\\\Desktop\\\\merge-csv.com__68d6b5a27ee3c.csv\")\n",
    "TEST_CSV = Path(\"C:\\\\Users\\\\hoseini\\\\Desktop\\\\Test_Forward_model.csv\")\n",
    "\n",
    "MODEL_DIR = BASE_DIR / \"best_model\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "PLOTS_DIR = BASE_DIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "ARTIFACTS = BASE_DIR / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# ===========================\n",
    "# Data Loading & Correction\n",
    "# ===========================\n",
    "def load_dataframe(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X = df.iloc[:, :3].copy()\n",
    "    y = df.iloc[:, -1].astype(float).copy()\n",
    "    return X, y, df\n",
    "\n",
    "def correct_targets(y: pd.Series):\n",
    "    y_corr = y.copy()\n",
    "    y_corr[y_corr < 0] = -y_corr[y_corr < 0]\n",
    "    y_corr = np.minimum(y_corr, 1.0)\n",
    "    return y_corr\n",
    "\n",
    "X_all, y_raw, _ = load_dataframe(TRAIN_CSV)\n",
    "y_all = correct_targets(y_raw)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all.values, y_all.values, test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# ===========================\n",
    " \n",
    "# ===========================\n",
    "def preprocess_inputs(X_train, X_val, cfg):\n",
    "    if cfg[\"c_encoding\"] == \"numeric\":\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        return scaler.transform(X_train), scaler.transform(X_val), {\"scaler\": scaler, \"encoder\": None}\n",
    "    else:\n",
    "       \n",
    "        enc = OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "        C_tr = enc.fit_transform(X_train[:, 2].reshape(-1,1))\n",
    "        C_va = enc.transform(X_val[:, 2].reshape(-1,1))\n",
    "        AB_tr = X_train[:, :2]\n",
    "        AB_va = X_val[:, :2]\n",
    "        scaler = StandardScaler().fit(AB_tr)\n",
    "        AB_tr_s = scaler.transform(AB_tr)\n",
    "        AB_va_s = scaler.transform(AB_va)\n",
    "        return np.hstack([AB_tr_s, C_tr]), np.hstack([AB_va_s, C_va]), {\"scaler\": scaler, \"encoder\": enc}\n",
    "\n",
    "# ===========================\n",
    "# Model Factory\n",
    "# ===========================\n",
    "def build_model(input_dim, cfg):\n",
    "    model = tf.keras.Sequential()\n",
    "    n_layers = cfg[\"n_layers\"]\n",
    "    units_list = cfg[\"units_per_layer\"]\n",
    "    \n",
    "    \n",
    "    if len(units_list) != n_layers:\n",
    "        units_list = [64] * n_layers\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        units = units_list[i]\n",
    "        act = cfg[\"activation\"]\n",
    "        l2 = regularizers.l2(cfg[\"l2\"]) if cfg[\"l2\"] > 0 else None\n",
    "        model.add(layers.Dense(units, activation=act, kernel_regularizer=l2))\n",
    "        if cfg[\"dropout\"] > 0:\n",
    "            model.add(layers.Dropout(cfg[\"dropout\"]))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=cfg[\"lr\"])\n",
    "    model.compile(optimizer=opt, loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# ===========================\n",
    "ACTIVATIONS = [\"relu\", \"tanh\"] \n",
    "\n",
    "def random_config():\n",
    "    n_layers = random.randint(1, 6)\n",
    "    units = [random.choice([32, 64, 128, 256, 512, 1024]) for _ in range(n_layers)]\n",
    "    return {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"units_per_layer\": units,\n",
    "        \"activation\": random.choice(ACTIVATIONS),\n",
    "        \"dropout\": round(random.uniform(0.0, 0.5), 2),\n",
    "        \"l2\": round(random.choice([0.0, 1e-6, 1e-5, 1e-4, 5e-4, 1e-3]), 6),\n",
    "        \"lr\": 10**random.uniform(-5, -2),\n",
    "        \"batch_size\": random.choice([128, 256, 512, 1024, 2048]),\n",
    "        \"epochs\": 400,\n",
    "        \"patience\": 40,\n",
    "        \"c_encoding\": random.choice([\"numeric\", \"onehot\"]),\n",
    "    }\n",
    "\n",
    "def mutate(cfg, p=0.3):\n",
    "    new_cfg = json.loads(json.dumps(cfg))\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"activation\"] = random.choice(ACTIVATIONS)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"dropout\"] = round(np.clip(new_cfg[\"dropout\"] + random.uniform(-0.1, 0.1), 0, 0.5), 2)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"l2\"] = round(random.choice([0.0, 1e-6, 1e-5, 1e-4, 5e-4, 1e-3]), 6)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"lr\"] = 10**random.uniform(-5, -2)\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"batch_size\"] = random.choice([128, 256, 512, 1024, 2048])\n",
    "    if random.random() < p:\n",
    "        nl = random.randint(1, 6)\n",
    "        new_cfg[\"n_layers\"] = nl\n",
    "        new_cfg[\"units_per_layer\"] = [random.choice([32, 64, 128, 256, 512, 1024]) for _ in range(nl)]\n",
    "    if random.random() < p:\n",
    "        new_cfg[\"c_encoding\"] = random.choice([\"numeric\", \"onehot\"])\n",
    "    \n",
    "    \n",
    "    nl = new_cfg[\"n_layers\"]\n",
    "    units = new_cfg[\"units_per_layer\"]\n",
    "    if len(units) != nl:\n",
    "        if len(units) > nl:\n",
    "            units = units[:nl]\n",
    "        else:\n",
    "            needed = nl - len(units)\n",
    "            last_unit = units[-1] if units else 64\n",
    "            units.extend([last_unit] * needed)\n",
    "        new_cfg[\"units_per_layer\"] = units\n",
    "    \n",
    "    return new_cfg\n",
    "\n",
    "def crossover(a, b):\n",
    "    child = {}\n",
    "    for k in a.keys():\n",
    "        child[k] = random.choice([a[k], b[k]])\n",
    "    \n",
    "    \n",
    "    nl = child[\"n_layers\"]\n",
    "    units = child[\"units_per_layer\"]\n",
    "    \n",
    "    if len(units) > nl:\n",
    "        units = units[:nl]\n",
    "    elif len(units) < nl:\n",
    "        needed = nl - len(units)\n",
    "        last_unit = units[-1] if units else 64\n",
    "        units.extend([last_unit] * needed)\n",
    "    \n",
    "    child[\"units_per_layer\"] = units\n",
    "    return child\n",
    "\n",
    "def evaluate_cfg(cfg, X_tr, y_tr, X_va, y_va):\n",
    "    try:\n",
    "        X_tr_p, X_va_p, _ = preprocess_inputs(X_tr, X_va, cfg)\n",
    "        model = build_model(X_tr_p.shape[1], cfg)\n",
    "        cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=cfg[\"patience\"], restore_best_weights=True)]\n",
    "        hist = model.fit(\n",
    "            X_tr_p, y_tr,\n",
    "            validation_data=(X_va_p, y_va),\n",
    "            epochs=cfg[\"epochs\"],\n",
    "            batch_size=cfg[\"batch_size\"],\n",
    "            verbose=0,\n",
    "            callbacks=cb\n",
    "        )\n",
    "        val_mse = float(min(hist.history[\"val_loss\"]))\n",
    "        best_epoch = int(np.argmin(hist.history[\"val_loss\"])) + 1\n",
    "        return val_mse, best_epoch\n",
    "    except Exception as e:\n",
    "        print(f\"error in configuration: {e}\")\n",
    "        return float('inf'), 0\n",
    "\n",
    "# ===========================\n",
    "# Run GA\n",
    "# ===========================\n",
    "POP_SIZE, N_GEN, ELITE_K, MUT_P = 24, 12, 5, 0.5\n",
    "population = [random_config() for _ in range(POP_SIZE)]\n",
    "hall = []\n",
    "\n",
    "for gen in range(N_GEN):\n",
    "    scored = []\n",
    "    for cfg in population:\n",
    "        val_mse, best_epoch = evaluate_cfg(cfg, X_train, y_train, X_val, y_val)\n",
    "        if val_mse < float('inf'):\n",
    "            scored.append((val_mse, best_epoch, cfg))\n",
    "    \n",
    "    if not scored:\n",
    "        print(\"no new configuration\")\n",
    "        population = [random_config() for _ in range(POP_SIZE)]\n",
    "        continue\n",
    "        \n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    best_gen = scored[0]\n",
    "    hall.append({\"gen\": gen, \"val_mse\": best_gen[0], \"best_epoch\": best_gen[1], \"cfg\": best_gen[2]})\n",
    "    print(f\"[Gen {gen}] best val MSE = {best_gen[0]:.6f} | encoding={best_gen[2]['c_encoding']}\")\n",
    "    new_pop = [scored[i][2] for i in range(min(ELITE_K, len(scored)))]\n",
    "    \n",
    "    while len(new_pop) < POP_SIZE:\n",
    "        if len(scored) >= 2:\n",
    "            a, b = random.sample(scored[:min(6, len(scored))], 2)\n",
    "            child = crossover(a[2], b[2])\n",
    "            child = mutate(child, p=MUT_P)\n",
    "            new_pop.append(child)\n",
    "        else:\n",
    "            new_pop.append(random_config())\n",
    "    \n",
    "    population = new_pop\n",
    "\n",
    "if hall:\n",
    "    hall.sort(key=lambda h: h[\"val_mse\"])\n",
    "    best_cfg = hall[0][\"cfg\"]\n",
    "    print(\"\\nBest config:\", json.dumps(best_cfg, indent=2))\n",
    "    with open(ARTIFACTS / \"best_config.json\", \"w\") as f:\n",
    "        json.dump(best_cfg, f, indent=2)\n",
    "\n",
    "    # ===========================\n",
    "    # Final Training on best config\n",
    "    # ===========================\n",
    "    X_train_p, X_val_p, preprocessors = preprocess_inputs(X_train, X_val, best_cfg)\n",
    "    best_model = build_model(X_train_p.shape[1], best_cfg)\n",
    "    cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=best_cfg[\"patience\"], restore_best_weights=True)]\n",
    "    hist = best_model.fit(\n",
    "        X_train_p, y_train,\n",
    "        validation_data=(X_val_p, y_val),\n",
    "        epochs=best_cfg[\"epochs\"],\n",
    "        batch_size=best_cfg[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "    best_model.save(MODEL_DIR / \"trained_model.h5\")\n",
    "    pd.DataFrame(hist.history).to_csv(ARTIFACTS / \"train_history.csv\", index=False)\n",
    "\n",
    "    # ===========================\n",
    "    # Plots\n",
    "    # ===========================\n",
    "    def save_tiff_plot(filename, fig):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(PLOTS_DIR / filename, dpi=600, format=\"tiff\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(hist.history[\"loss\"], label=\"Train MSE\")\n",
    "    plt.plot(hist.history[\"val_loss\"], label=\"Validation MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Training vs Validation MSE\")\n",
    "    plt.legend()\n",
    "    save_tiff_plot(\"mse_train_val.tiff\", fig1)\n",
    "\n",
    "    # ===========================\n",
    "    # Test Evaluation\n",
    "    # ===========================\n",
    "    def load_test(csv_path: Path, cfg, preprocessors):\n",
    "        X_t, y_t, _ = load_dataframe(csv_path)\n",
    "        y_t_corr = correct_targets(y_t)\n",
    "        if cfg[\"c_encoding\"] == \"numeric\":\n",
    "            scaler = preprocessors[\"scaler\"]\n",
    "            X_t_p = scaler.transform(X_t.values)\n",
    "        else:\n",
    "            scaler, enc = preprocessors[\"scaler\"], preprocessors[\"encoder\"]\n",
    "            C_te = enc.transform(X_t.values[:, 2].reshape(-1,1))\n",
    "            AB_te = scaler.transform(X_t.values[:, :2])\n",
    "            X_t_p = np.hstack([AB_te, C_te])\n",
    "        return X_t_p, y_t_corr.values\n",
    "\n",
    "    X_test_p, y_test_corr = load_test(TEST_CSV, best_cfg, preprocessors)\n",
    "    y_pred = best_model.predict(X_test_p, verbose=0).ravel()\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(y_test_corr, label=\"True\", color=\"black\", linewidth=1.5)\n",
    "    plt.plot(y_pred, label=\"Predicted\", color=\"red\", linewidth=1.5)\n",
    "    plt.scatter(range(len(y_test_corr)), y_test_corr, s=14, c=\"black\", marker=\"o\")\n",
    "    plt.scatter(range(len(y_pred)), y_pred, s=14, c=\"red\", marker=\"x\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"D\")\n",
    "    plt.title(\"Test Set: True vs Predicted (continuous + points)\")\n",
    "    plt.legend()\n",
    "    save_tiff_plot(\"test_true_vs_pred_continuous.tiff\", fig2)\n",
    "\n",
    "    test_mse = mean_squared_error(y_test_corr, y_pred)\n",
    "\n",
    "    # ===========================\n",
    "    # Final Report JSON\n",
    "    # ===========================\n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"train_size\": int(X_train.shape[0]),\n",
    "        \"val_size\": int(X_val.shape[0]),\n",
    "        \"test_size\": int(len(y_test_corr)),\n",
    "        \"input_columns\": [\"A\",\"B\",\"C\"],\n",
    "        \"target_column\": \"D\",\n",
    "        \"label_corrections\": {\n",
    "            \"negatives_reflected_train\": int((y_raw < 0).sum()),\n",
    "            \"clipped_above_1_train\": int((y_raw > 1).sum()),\n",
    "            \"negatives_reflected_test\": int((y_test_corr<0).sum()),\n",
    "            \"clipped_above_1_test\": int((y_test_corr>1).sum())\n",
    "        },\n",
    "        \"best_config\": best_cfg,\n",
    "        \"best_val_mse\": float(min(hist.history[\"val_loss\"])),\n",
    "        \"early_stopped_epoch\": int(np.argmin(hist.history[\"val_loss\"])) + 1,\n",
    "        \"test_mse\": float(test_mse),\n",
    "        \"artifacts\": {\n",
    "            \"model_h5\": str((MODEL_DIR / \"trained_model.h5\").resolve()),\n",
    "            \"best_config_json\": str((ARTIFACTS / \"best_config.json\").resolve()),\n",
    "            \"train_history_csv\": str((ARTIFACTS / \"train_history.csv\").resolve()),\n",
    "            \"mse_plot_tiff\": str((PLOTS_DIR / \"mse_train_val.tiff\").resolve()),\n",
    "            \"test_plot_tiff\": str((PLOTS_DIR / \"test_true_vs_pred_continuous.tiff\").resolve())\n",
    "        }\n",
    "    }\n",
    "    with open(BASE_DIR / \"summary_report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(\"✅ Training finished. Test MSE =\", test_mse)\n",
    "else:\n",
    "    print(\"❌ no configuration !\")\n",
    "\n",
    "print(\"🎯 finish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
